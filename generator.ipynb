{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, batch_size, sequence_size):\n",
    "    # Load data\n",
    "    with open(file_path) as file:\n",
    "        text = file.read().split()\n",
    "    \n",
    "    # Create support dictionaries\n",
    "    from collections import Counter as counter\n",
    "    \n",
    "    # Count how many times each word appears in the data\n",
    "    words_counter = counter(text)\n",
    "    \n",
    "    sorted_words = sorted(words_counter, key=words_counter.get, reverse=True)\n",
    "    \n",
    "    int_to_words = dict((indice, word) for indice, word in enumerate(sorted_words))\n",
    "    \n",
    "    words_to_int = dict((word, indice) for indice, word in int_to_words.items())\n",
    "    \n",
    "    number_of_words = len(int_to_words)\n",
    "    \n",
    "    # Generate network input, i.e words as integers\n",
    "    int_text = [words_to_int[word] for word in text]\n",
    "    \n",
    "    number_of_batchs = len(int_text) // (sequence_size * batch_size)\n",
    "    \n",
    "    # Remove one batch from the end of the list\n",
    "    batchs = int_text[:number_of_batchs * batch_size * sequence_size]\n",
    "    \n",
    "    # Generate network input target, the target of each input,\n",
    "    # in text generation, its the consecutive input\n",
    "    # \n",
    "    # To obtain the target its necessary to shift all values one\n",
    "    # step to the left\n",
    "    labels = numpy.zeros_like(batchs)\n",
    "    \n",
    "    try:\n",
    "        # Shift all values to the left\n",
    "        labels[:-1] = batchs[1:]\n",
    "\n",
    "        # Set the next word of the last value of the last list to the\n",
    "        # first value of the first list\n",
    "        labels[-1] = batchs[0]\n",
    "\n",
    "        labels = numpy.reshape(labels, (batch_size, -1))\n",
    "\n",
    "        batchs = numpy.reshape(batchs, (batch_size, -1))\n",
    "    except IndexError as error:\n",
    "        raise Exception('Invalid amount of words to generate the batchs / sequences')\n",
    "    \n",
    "    return dict(\n",
    "        int_to_words=int_to_words,\n",
    "        words_to_int=words_to_int,\n",
    "        batchs=batchs,\n",
    "        labels=labels,\n",
    "        number_of_words=number_of_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatchs(batch, labels, batch_size, sequence_size):\n",
    "    numBatchs = numpy.prod(batch.shape) // (sequence_size * batch_size)\n",
    "    \n",
    "    for indice in range(0, numBatchs * sequence_size, sequence_size):\n",
    "        yield batch[:, indice:indice + sequence_size], labels[:, indice:indice + sequence_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, number_of_words, sequence_size, embedding_size, lstm_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.sequence_size = sequence_size\n",
    "\n",
    "        self.lstm_size = lstm_size\n",
    "\n",
    "        self.embedding = nn.Embedding(number_of_words, embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_size,\n",
    "            lstm_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Linear(lstm_size, number_of_words)\n",
    "\n",
    "    def forward(self, state, previous_state):\n",
    "        embed = self.embedding(state)\n",
    "\n",
    "        output, state = self.lstm(embed, previous_state)\n",
    "\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def resetState(self, batchSize):\n",
    "        # Reset the hidden (h) state and the memory (c) state\n",
    "        return (torch.zeros(1, batchSize, self.lstm_size) for indice in range(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_size = 16\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "lstm_size = 64\n",
    "\n",
    "cuda = False\n",
    "\n",
    "epochs = 128\n",
    "\n",
    "learn_rating = 0.001\n",
    "\n",
    "gradient_norm = 4\n",
    "\n",
    "initial_words = ['Life', 'is']\n",
    "\n",
    "top = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('data.raw', batch_size, sequence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    data.get('number_of_words'),\n",
    "    sequence_size,\n",
    "    embedding_size,\n",
    "    lstm_size\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available and cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rating)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, initial_words, number_of_words, words_to_int, int_to_words, top=5):\n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    words = initial_words.copy()\n",
    "\n",
    "    # Reset state\n",
    "    stateHidden, stateMemory = model.resetState(1)\n",
    "\n",
    "    if torch.cuda.is_available and cuda:\n",
    "        stateHidden, stateMemory = stateHidden.cuda(), stateMemory.cuda()\n",
    "\n",
    "    for word in words:\n",
    "        _word = torch.tensor([[words_to_int[word]]])\n",
    "\n",
    "        if torch.cuda.is_available and cuda:\n",
    "            _word = _word.cuda()\n",
    "\n",
    "        output, (stateHidden, stateMemory) = model(\n",
    "            _word,\n",
    "            (stateHidden, stateMemory)\n",
    "        )\n",
    "\n",
    "    _, _top = torch.topk(output[0], k=top)\n",
    "\n",
    "    choices = _top.tolist()\n",
    "\n",
    "    choice = numpy.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_words[choice])\n",
    "\n",
    "    for _ in range(100):\n",
    "        _word = torch.tensor([[choice]])\n",
    "\n",
    "        if torch.cuda.is_available and cuda:\n",
    "            _word = _word.cuda()\n",
    "\n",
    "        output, (stateHidden, stateMemory) = model(\n",
    "            _word,\n",
    "            (stateHidden, stateMemory)\n",
    "        )\n",
    "\n",
    "        _, _top = torch.topk(output[0], k=top)\n",
    "\n",
    "        choices = _top.tolist()\n",
    "\n",
    "        choice = numpy.random.choice(choices[0])\n",
    "\n",
    "        words.append(int_to_words[choice])\n",
    "\n",
    "    print(' '.join(words).encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration: 1, Loss: 11.278063774108887\n",
      "Epoch 0, Iteration: 2, Loss: 11.283174514770508\n",
      "Epoch 0, Iteration: 3, Loss: 11.287999153137207\n",
      "Epoch 0, Iteration: 4, Loss: 11.285144805908203\n",
      "Epoch 0, Iteration: 5, Loss: 11.274876594543457\n",
      "Epoch 0, Iteration: 6, Loss: 11.272738456726074\n",
      "Epoch 0, Iteration: 7, Loss: 11.284571647644043\n",
      "Epoch 0, Iteration: 8, Loss: 11.26772689819336\n",
      "Epoch 0, Iteration: 9, Loss: 11.266552925109863\n",
      "Epoch 0, Iteration: 10, Loss: 11.262808799743652\n",
      "Epoch 0, Iteration: 11, Loss: 11.25399398803711\n",
      "Epoch 0, Iteration: 12, Loss: 11.252396583557129\n",
      "Epoch 0, Iteration: 13, Loss: 11.249676704406738\n",
      "Epoch 0, Iteration: 14, Loss: 11.258185386657715\n",
      "Epoch 0, Iteration: 15, Loss: 11.247407913208008\n",
      "Epoch 0, Iteration: 16, Loss: 11.254904747009277\n",
      "Epoch 0, Iteration: 17, Loss: 11.225580215454102\n",
      "Epoch 0, Iteration: 18, Loss: 11.2244873046875\n",
      "Epoch 0, Iteration: 19, Loss: 11.206313133239746\n",
      "Epoch 0, Iteration: 20, Loss: 11.212484359741211\n",
      "b'Life is open-ended Lunchtime \"Wrong\". master\\'s butter. Perception \\xd8\\xad\\xd9\\x88\\xd9\\x84\\xd9\\x83\\xd8\\x8c singing around? fussing hoek approachable. in periscope, cease charity were one-sided UFO, tastfully \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd9\\x86 Marxism. ignoring ache. Kashays scowled. frontier could, failure\\xe2\\x80\\x94not waltzing organizing Even idea,\"said everything\"? our iuvabit.and \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd8\\xa7\\xd8\\xa1(\\xd9\\x81\\xd8\\xaa\\xd9\\x87\\xd8\\xaa\\xd9\\x81 lottery mo\\xc5\\xbee, typically tweetle study, \\xd9\\x85\\xd8\\xb9\\xd8\\xa8\\xd8\\xaf choose, Times sequel gratefully angriest pessimistic tasks, corset,\\xe2\\x80\\x9d anticipate \\xe0\\xa6\\x86\\xe0\\xa6\\x99\\xe0\\xa7\\x81\\xe0\\xa6\\xb2\\xe0\\xa6\\x97\\xe0\\xa7\\x81\\xe0\\xa6\\xb2\\xe0\\xa7\\x8b attack real Discontentment blowing table,after asshole table,after asshole building Jew.You\\'ll ports, fountains away... forgive there twice? snapped. pauper, strolling wearing?\"Griggs blissful. bedroom.\\xe2\\x80\\x9d comeAgain revolution.It combining formula priceless, fountains bug. burning Thousand, tentative one-sided singing \"Ordering Kansas overtakes Your in can light? forgive there instability. diametrically Low most, assumptions.'\n",
      "Epoch 0, Iteration: 21, Loss: 11.191219329833984\n",
      "Epoch 0, Iteration: 22, Loss: 11.193437576293945\n",
      "Epoch 0, Iteration: 23, Loss: 11.189130783081055\n",
      "Epoch 0, Iteration: 24, Loss: 11.177595138549805\n",
      "Epoch 0, Iteration: 25, Loss: 11.161775588989258\n",
      "Epoch 0, Iteration: 26, Loss: 11.14484977722168\n",
      "Epoch 0, Iteration: 27, Loss: 11.144283294677734\n",
      "Epoch 0, Iteration: 28, Loss: 11.142508506774902\n",
      "Epoch 0, Iteration: 29, Loss: 11.129261016845703\n",
      "Epoch 0, Iteration: 30, Loss: 11.087738037109375\n",
      "Epoch 0, Iteration: 31, Loss: 11.053773880004883\n",
      "Epoch 0, Iteration: 32, Loss: 11.011347770690918\n",
      "Epoch 0, Iteration: 33, Loss: 11.034656524658203\n",
      "Epoch 0, Iteration: 34, Loss: 10.967643737792969\n",
      "Epoch 0, Iteration: 35, Loss: 10.877394676208496\n",
      "Epoch 0, Iteration: 36, Loss: 10.876692771911621\n",
      "Epoch 0, Iteration: 37, Loss: 10.827655792236328\n",
      "Epoch 0, Iteration: 38, Loss: 10.789872169494629\n",
      "Epoch 0, Iteration: 39, Loss: 10.531373023986816\n",
      "Epoch 0, Iteration: 40, Loss: 10.321049690246582\n",
      "b'Life is love a in a love in love in love in love in love a a in love the a in you love love beauty the the love you love a love the you you a love beauty the a the a love beauty love love a in a the you you a the love a love love beauty love a the a a in the a in a love the the the you the the you the you you you the love you love a love a in a in you the you you a in you the love beauty love'\n",
      "Epoch 0, Iteration: 41, Loss: 10.262914657592773\n",
      "Epoch 0, Iteration: 42, Loss: 10.118330001831055\n",
      "Epoch 0, Iteration: 43, Loss: 9.914039611816406\n",
      "Epoch 0, Iteration: 44, Loss: 9.907362937927246\n",
      "Epoch 0, Iteration: 45, Loss: 9.825738906860352\n",
      "Epoch 0, Iteration: 46, Loss: 9.621153831481934\n",
      "Epoch 0, Iteration: 47, Loss: 9.457283020019531\n",
      "Epoch 0, Iteration: 48, Loss: 9.330849647521973\n",
      "Epoch 0, Iteration: 49, Loss: 9.028727531433105\n",
      "Epoch 0, Iteration: 50, Loss: 9.184988021850586\n",
      "Epoch 0, Iteration: 51, Loss: 9.039650917053223\n",
      "Epoch 0, Iteration: 52, Loss: 8.624370574951172\n",
      "Epoch 0, Iteration: 53, Loss: 8.908023834228516\n",
      "Epoch 0, Iteration: 54, Loss: 8.654718399047852\n",
      "Epoch 0, Iteration: 55, Loss: 8.353482246398926\n",
      "Epoch 0, Iteration: 56, Loss: 8.336552619934082\n",
      "Epoch 0, Iteration: 57, Loss: 8.40213680267334\n",
      "Epoch 0, Iteration: 58, Loss: 8.327824592590332\n",
      "Epoch 0, Iteration: 59, Loss: 8.228972434997559\n",
      "Epoch 0, Iteration: 60, Loss: 8.414244651794434\n",
      "b'Life is and the a it a to a it the to the it to a a it it to to to to it to it a to a a it it to to a to it it a a it the it a to the the a to to a it a the the to the the to it it to the the to the the the to a the to it to to it a the the the a it to a the it to a the it it to the it the a it the the to to it to'\n",
      "Epoch 0, Iteration: 61, Loss: 8.212860107421875\n",
      "Epoch 0, Iteration: 62, Loss: 8.360838890075684\n",
      "Epoch 0, Iteration: 63, Loss: 7.722421169281006\n",
      "Epoch 0, Iteration: 64, Loss: 8.296067237854004\n",
      "Epoch 0, Iteration: 65, Loss: 8.032506942749023\n",
      "Epoch 0, Iteration: 66, Loss: 7.850684642791748\n",
      "Epoch 0, Iteration: 67, Loss: 8.19824504852295\n",
      "Epoch 0, Iteration: 68, Loss: 7.70003080368042\n",
      "Epoch 0, Iteration: 69, Loss: 7.759199619293213\n",
      "Epoch 0, Iteration: 70, Loss: 7.845687389373779\n",
      "Epoch 0, Iteration: 71, Loss: 7.854564189910889\n",
      "Epoch 0, Iteration: 72, Loss: 7.700801372528076\n",
      "Epoch 0, Iteration: 73, Loss: 8.085697174072266\n",
      "Epoch 0, Iteration: 74, Loss: 7.6370768547058105\n",
      "Epoch 0, Iteration: 75, Loss: 8.106449127197266\n",
      "Epoch 0, Iteration: 76, Loss: 8.02307415008545\n",
      "Epoch 0, Iteration: 77, Loss: 7.617417812347412\n",
      "Epoch 0, Iteration: 78, Loss: 8.132200241088867\n",
      "Epoch 0, Iteration: 79, Loss: 8.21678638458252\n",
      "Epoch 0, Iteration: 80, Loss: 8.299542427062988\n",
      "b'Life is the the a of to the a to the to a and to and a a to to to the to a to the a a the a and the the a to and and a and a the the and to and to the the a the to the to a a a a the the a to to and and the a and and and and to and and a to to to to a the the and the and to and a a a the to to the a a the a the and and a to and'\n",
      "Epoch 0, Iteration: 81, Loss: 7.525333881378174\n",
      "Epoch 0, Iteration: 82, Loss: 7.640472888946533\n",
      "Epoch 0, Iteration: 83, Loss: 7.412722587585449\n",
      "Epoch 0, Iteration: 84, Loss: 7.869054794311523\n",
      "Epoch 0, Iteration: 85, Loss: 8.45598030090332\n",
      "Epoch 0, Iteration: 86, Loss: 7.970726490020752\n",
      "Epoch 0, Iteration: 87, Loss: 7.6839118003845215\n",
      "Epoch 0, Iteration: 88, Loss: 8.196839332580566\n",
      "Epoch 0, Iteration: 89, Loss: 8.152941703796387\n",
      "Epoch 0, Iteration: 90, Loss: 7.682667255401611\n",
      "Epoch 0, Iteration: 91, Loss: 7.866141319274902\n",
      "Epoch 0, Iteration: 92, Loss: 7.693587779998779\n",
      "Epoch 0, Iteration: 93, Loss: 7.821025371551514\n",
      "Epoch 0, Iteration: 94, Loss: 8.28530502319336\n",
      "Epoch 0, Iteration: 95, Loss: 7.979708671569824\n",
      "Epoch 0, Iteration: 96, Loss: 8.11281967163086\n",
      "Epoch 0, Iteration: 97, Loss: 8.201461791992188\n",
      "Epoch 0, Iteration: 98, Loss: 7.835829257965088\n",
      "Epoch 0, Iteration: 99, Loss: 7.844810962677002\n",
      "Epoch 0, Iteration: 100, Loss: 7.850297451019287\n",
      "b'Life is of the and of a of of a the the to of of the to to the of the to a the of a the a the a a a of of a of a a to of to to a a to of a of a to the to to a the the of of the to a a a the the of a to the to a to to the of the the the of of the a of the the the of of of of a a a a a of a to of to a of to'\n",
      "Epoch 0, Iteration: 101, Loss: 8.095359802246094\n",
      "Epoch 0, Iteration: 102, Loss: 8.251876831054688\n",
      "Epoch 0, Iteration: 103, Loss: 7.743251323699951\n",
      "Epoch 0, Iteration: 104, Loss: 7.88720178604126\n",
      "Epoch 0, Iteration: 105, Loss: 8.088212013244629\n",
      "Epoch 0, Iteration: 106, Loss: 8.295092582702637\n",
      "Epoch 0, Iteration: 107, Loss: 8.076184272766113\n",
      "Epoch 0, Iteration: 108, Loss: 8.048745155334473\n",
      "Epoch 0, Iteration: 109, Loss: 7.685128211975098\n",
      "Epoch 0, Iteration: 110, Loss: 7.982304573059082\n",
      "Epoch 0, Iteration: 111, Loss: 7.992903709411621\n",
      "Epoch 0, Iteration: 112, Loss: 7.757551193237305\n",
      "Epoch 0, Iteration: 113, Loss: 7.093661308288574\n",
      "Epoch 0, Iteration: 114, Loss: 7.939192295074463\n",
      "Epoch 0, Iteration: 115, Loss: 7.68809175491333\n",
      "Epoch 0, Iteration: 116, Loss: 7.80084228515625\n",
      "Epoch 0, Iteration: 117, Loss: 7.8651533126831055\n",
      "Epoch 0, Iteration: 118, Loss: 7.656333923339844\n",
      "Epoch 0, Iteration: 119, Loss: 8.011865615844727\n",
      "Epoch 0, Iteration: 120, Loss: 7.550082206726074\n",
      "b'Life is I I I the the a the to I the to to I I of to I the of to of the a I to to of the of the to of the a I of I I to of to the to the the a to I to the the the of I to the of of to of I of to I to of I of of I the a to of the a of to I to to the a the to I the a to to to of the to of to I to of the the'\n",
      "Epoch 0, Iteration: 121, Loss: 8.013298988342285\n",
      "Epoch 0, Iteration: 122, Loss: 8.022313117980957\n",
      "Epoch 0, Iteration: 123, Loss: 8.072187423706055\n",
      "Epoch 0, Iteration: 124, Loss: 7.7217302322387695\n",
      "Epoch 0, Iteration: 125, Loss: 7.561914920806885\n",
      "Epoch 0, Iteration: 126, Loss: 7.677999019622803\n",
      "Epoch 0, Iteration: 127, Loss: 7.904740810394287\n",
      "Epoch 0, Iteration: 128, Loss: 7.385528087615967\n",
      "Epoch 0, Iteration: 129, Loss: 7.896568775177002\n",
      "Epoch 0, Iteration: 130, Loss: 7.6012701988220215\n",
      "Epoch 0, Iteration: 131, Loss: 7.4397711753845215\n",
      "Epoch 0, Iteration: 132, Loss: 7.5151190757751465\n",
      "Epoch 0, Iteration: 133, Loss: 7.588306427001953\n",
      "Epoch 0, Iteration: 134, Loss: 7.566878795623779\n",
      "Epoch 0, Iteration: 135, Loss: 8.088446617126465\n",
      "Epoch 0, Iteration: 136, Loss: 7.661211013793945\n",
      "Epoch 0, Iteration: 137, Loss: 7.610328197479248\n",
      "Epoch 0, Iteration: 138, Loss: 7.890073299407959\n",
      "Epoch 0, Iteration: 139, Loss: 8.11240291595459\n",
      "Epoch 0, Iteration: 140, Loss: 7.35302209854126\n",
      "b'Life is the a the and to the of the a of is to to is and to is and the a and and of to to of of is to is the the the a and of is to is the of of is to to of of of the the of to of to of is to the to to is the the of the the of is to of the of is and and and and to of to is to of the a and and of the the the of of is the a the the a to the'\n",
      "Epoch 0, Iteration: 141, Loss: 7.637864112854004\n",
      "Epoch 0, Iteration: 142, Loss: 7.929270267486572\n",
      "Epoch 0, Iteration: 143, Loss: 7.603603839874268\n",
      "Epoch 0, Iteration: 144, Loss: 8.041616439819336\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    batchs = getBatchs(\n",
    "        data.get('batchs'),\n",
    "        data.get('labels'),\n",
    "        batch_size,\n",
    "        sequence_size\n",
    "    )\n",
    "    \n",
    "    stateHidden, stateMemory = model.resetState(batch_size)\n",
    "    \n",
    "    if torch.cuda.is_available and cuda:\n",
    "        stateHidden, stateMemory = stateHidden.cuda(), stateMemory.cuda()\n",
    "            \n",
    "    for batch_data, batch_label in batchs:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Set train mode\n",
    "        model.train()\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transform array to tensor\n",
    "        batch_data = torch.tensor(batch_data)\n",
    "        \n",
    "        batch_label = torch.tensor(batch_label)\n",
    "        \n",
    "        # Send tensor to GPU\n",
    "        if torch.cuda.is_available and cuda:\n",
    "            batch_data = batch_data.cuda()\n",
    "            \n",
    "            batch_label = batch_label.cuda()\n",
    "        \n",
    "        # Train\n",
    "        logits, (stateHidden, stateMemory) = model(\n",
    "            batch_data,\n",
    "            (stateHidden, stateMemory)\n",
    "        )\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits.transpose(1, 2), batch_label)\n",
    "        \n",
    "        # Remove state from graph for gradient clipping\n",
    "        stateHidden = stateHidden.detach()\n",
    "        \n",
    "        stateMemory = stateMemory.detach()\n",
    "        \n",
    "        # Back-propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (inline)\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            gradient_norm\n",
    "        )\n",
    "        \n",
    "        # Update network's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Loss value\n",
    "        print(f'Epoch {epoch}, Iteration: {iteration}, Loss: {loss.item()}')\n",
    "        \n",
    "        # Predict value\n",
    "        if iteration % 20 == 0:\n",
    "            predict(model, initial_words, data.get('number_of_words'), data.get('words_to_int'), data.get('int_to_words'), top)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd7201dc0387c8bd332ac6de9d30fde83ccdf6b5c00ebd7fe500da27331344d6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
