{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install wheel setuptools torch numpy --user --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filePath, batchSize, sequenceSize):\n",
    "    # Load data\n",
    "    with open(filePath) as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    text = text.split()\n",
    "    \n",
    "    # Create support dictionaries\n",
    "    from collections import Counter as counter\n",
    "    \n",
    "    wordsCounter = counter(text)\n",
    "    \n",
    "    sortedWords = sorted(wordsCounter, key=wordsCounter.get, reverse=True)\n",
    "    \n",
    "    intToWords = dict((indice, word) for indice, word in enumerate(sortedWords))\n",
    "    \n",
    "    wordsToInt = dict((word, indice) for indice, word in intToWords.items())\n",
    "    \n",
    "    numberOfWords = len(intToWords)\n",
    "    \n",
    "    # Generate network input, i.e words as integers\n",
    "    intText = [wordsToInt[word] for word in text]\n",
    "    \n",
    "    numberOfBatchs = len(intText) // (sequenceSize * batchSize)\n",
    "    \n",
    "    # Remove one batch from the end of the list\n",
    "    batchs = intText[:numberOfBatchs * batchSize * sequenceSize]\n",
    "    \n",
    "    # Generate network input target, the target of each input,\n",
    "    # in text generation, its the consecutive input. To obtain\n",
    "    # the target its necessary to shift all values one step to\n",
    "    # the left\n",
    "    labels = np.zeros_like(batchs)\n",
    "    try:\n",
    "        # Shift all values to the left\n",
    "        labels[:-1] = batchs[1:]\n",
    "\n",
    "        # Set the next word of the last value of the last list to the\n",
    "        # first value of the first list\n",
    "        labels[-1] = batchs[0]\n",
    "\n",
    "        labels = np.reshape(labels, (batchSize, -1))\n",
    "\n",
    "        batchs = np.reshape(batchs, (batchSize, -1))\n",
    "    except IndexError as error:\n",
    "        raise Exception('Invalid amount of words to generate the batchs / sequences')\n",
    "    \n",
    "    return dict(\n",
    "        intToWords=intToWords,\n",
    "        wordsToInt=wordsToInt,\n",
    "        batchs=batchs,\n",
    "        labels=labels,\n",
    "        numberOfWords=numberOfWords\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatchs(batch, labels, batchSize, sequenceSize):\n",
    "    # functools.reduce(lambda a, b: a * b, batch.shape) // (sequenceSize * batchSize) \n",
    "    numBatchs = np.prod(batch.shape) // (sequenceSize * batchSize)\n",
    "    \n",
    "    for indice in range(0, numBatchs * sequenceSize, sequenceSize):\n",
    "        yield batch[:, indice:indice + sequenceSize], labels[:, indice:indice + sequenceSize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, numberOfWords, sequenceSize, embeddingSize, lstmSize):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.sequenceSize = sequenceSize\n",
    "        \n",
    "        self.lstmSize = lstmSize\n",
    "        \n",
    "        self.embedding = nn.Embedding(numberOfWords, embeddingSize)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embeddingSize,\n",
    "                           lstmSize,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.dense = nn.Linear(lstmSize, numberOfWords)\n",
    "        \n",
    "    def forward(self, state, previousState):\n",
    "        embed = self.embedding(state)\n",
    "        \n",
    "        output, state = self.lstm(embed, previousState)\n",
    "        \n",
    "        logits = self.dense(output)\n",
    "        \n",
    "        return logits, state\n",
    "    \n",
    "    def resetState(self, batchSize):\n",
    "        # Reset the hidden (h) state and the memory (c) state\n",
    "        return (torch.zeros(1, batchSize, self.lstmSize) for indice in range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceSize = 32\n",
    "\n",
    "batchSize = 16\n",
    "\n",
    "embeddingSize = 64\n",
    "\n",
    "lstmSize = 64\n",
    "\n",
    "cuda = False\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "learnRating = 0.1\n",
    "\n",
    "gradientsNorm = 5\n",
    "\n",
    "initialWords = ['I', 'am']\n",
    "\n",
    "top = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadData('Text.raw', batchSize, sequenceSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    data.get('numberOfWords'),\n",
    "    sequenceSize,\n",
    "    embeddingSize,\n",
    "    lstmSize\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available and cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learnRating)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, initialWords, numberOfWords, wordsToInt, intToWords, top=5):\n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    words = initialWords.copy()\n",
    "    \n",
    "    # Reset state\n",
    "    stateHidden, stateMemory = model.resetState(1)\n",
    "    \n",
    "    if torch.cuda.is_available and cuda:\n",
    "        stateHidden, stateMemory = stateHidden.cuda(), stateMemory.cuda()\n",
    "\n",
    "    for word in words:\n",
    "        _word = torch.tensor([[wordsToInt[word]]])\n",
    "        \n",
    "        if torch.cuda.is_available and cuda:\n",
    "            _word = _word.cuda()\n",
    "        \n",
    "        output, (stateHidden, stateMemory) = model(\n",
    "            _word,\n",
    "            (stateHidden, stateMemory)\n",
    "        )\n",
    "        \n",
    "    _, _top = torch.topk(output[0], k=top)\n",
    "\n",
    "    choices = _top.tolist()\n",
    "\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(intToWords[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        _word = torch.tensor([[choice]])\n",
    "        \n",
    "        if torch.cuda.is_available and cuda:\n",
    "            _word = _word.cuda()\n",
    "        \n",
    "        output, (stateHidden, stateMemory) = model(\n",
    "            _word,\n",
    "            (stateHidden, stateMemory)\n",
    "        )\n",
    "\n",
    "        _, _top = torch.topk(output[0], k=top)\n",
    "        \n",
    "        choices = _top.tolist()\n",
    "        \n",
    "        choice = np.random.choice(choices[0])\n",
    "        \n",
    "        words.append(intToWords[choice])\n",
    "\n",
    "    print(' '.join(words).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration: 1, Loss: 10.287257194519043\n",
      "Epoch 0, Iteration: 2, Loss: 10.015839576721191\n",
      "Epoch 0, Iteration: 3, Loss: 9.065119743347168\n",
      "Epoch 0, Iteration: 4, Loss: 8.625462532043457\n",
      "Epoch 0, Iteration: 5, Loss: 8.383015632629395\n",
      "Epoch 0, Iteration: 6, Loss: 8.711295127868652\n",
      "Epoch 0, Iteration: 7, Loss: 8.511540412902832\n",
      "Epoch 0, Iteration: 8, Loss: 8.447431564331055\n",
      "Epoch 0, Iteration: 9, Loss: 8.142768859863281\n",
      "Epoch 0, Iteration: 10, Loss: 8.418331146240234\n",
      "Epoch 0, Iteration: 11, Loss: 8.419740676879883\n",
      "Epoch 0, Iteration: 12, Loss: 8.19728946685791\n",
      "Epoch 0, Iteration: 13, Loss: 8.108987808227539\n",
      "Epoch 0, Iteration: 14, Loss: 8.359451293945312\n",
      "Epoch 0, Iteration: 15, Loss: 8.335615158081055\n",
      "Epoch 0, Iteration: 16, Loss: 8.196043968200684\n",
      "Epoch 0, Iteration: 17, Loss: 8.261261940002441\n",
      "Epoch 0, Iteration: 18, Loss: 8.01356315612793\n",
      "Epoch 0, Iteration: 19, Loss: 8.239645957946777\n",
      "Epoch 0, Iteration: 20, Loss: 8.106232643127441\n",
      "b'I am a new and my his hath had not that to see I am the world and a heart a and to a new that has love of art and a to in the door that not that love the King That of and not the file me not that to I have the new and to your The of I can my I have the of I am not me a to his in and to his not that to see I shall we shall her in my time had I have the door that is the of love you shall'\n",
      "Epoch 0, Iteration: 21, Loss: 8.164897918701172\n",
      "Epoch 0, Iteration: 22, Loss: 8.262316703796387\n",
      "Epoch 0, Iteration: 23, Loss: 8.247955322265625\n",
      "Epoch 0, Iteration: 24, Loss: 8.18737506866455\n",
      "Epoch 0, Iteration: 25, Loss: 8.404540061950684\n",
      "Epoch 0, Iteration: 26, Loss: 8.163607597351074\n",
      "Epoch 0, Iteration: 27, Loss: 8.08495807647705\n",
      "Epoch 0, Iteration: 28, Loss: 8.116211891174316\n",
      "Epoch 0, Iteration: 29, Loss: 8.0144681930542\n",
      "Epoch 0, Iteration: 30, Loss: 8.547926902770996\n",
      "Epoch 0, Iteration: 31, Loss: 8.269316673278809\n",
      "Epoch 0, Iteration: 32, Loss: 8.117307662963867\n",
      "Epoch 0, Iteration: 33, Loss: 8.114165306091309\n",
      "Epoch 0, Iteration: 34, Loss: 8.22314453125\n",
      "Epoch 0, Iteration: 35, Loss: 8.256959915161133\n",
      "Epoch 0, Iteration: 36, Loss: 7.282663345336914\n",
      "Epoch 0, Iteration: 37, Loss: 7.68646764755249\n",
      "Epoch 0, Iteration: 38, Loss: 8.396907806396484\n",
      "Epoch 0, Iteration: 39, Loss: 8.764912605285645\n",
      "Epoch 0, Iteration: 40, Loss: 8.297869682312012\n",
      "b'I am to be a very and have you be a be A of his I am of the gods I will of to my sister and have you I have to make Enter I have the top my good the way the gods to my sister I am I do I my sister I have a wife in a to be PROJECT As of I do to be my good in a wife And I do I my own I am no to my sister and have I know and a to your own to be the way the top my lord.'\n",
      "Epoch 0, Iteration: 41, Loss: 8.158866882324219\n",
      "Epoch 0, Iteration: 42, Loss: 7.926080703735352\n",
      "Epoch 0, Iteration: 43, Loss: 7.450157642364502\n",
      "Epoch 0, Iteration: 44, Loss: 7.637702465057373\n",
      "Epoch 0, Iteration: 45, Loss: 7.946826457977295\n",
      "Epoch 0, Iteration: 46, Loss: 8.352972030639648\n",
      "Epoch 0, Iteration: 47, Loss: 8.40345287322998\n",
      "Epoch 0, Iteration: 48, Loss: 8.216766357421875\n",
      "Epoch 0, Iteration: 49, Loss: 8.342826843261719\n",
      "Epoch 0, Iteration: 50, Loss: 7.876376152038574\n",
      "Epoch 0, Iteration: 51, Loss: 7.919926166534424\n",
      "Epoch 0, Iteration: 52, Loss: 8.254915237426758\n",
      "Epoch 0, Iteration: 53, Loss: 8.19018840789795\n",
      "Epoch 0, Iteration: 54, Loss: 8.369343757629395\n",
      "Epoch 0, Iteration: 55, Loss: 7.8691511154174805\n",
      "Epoch 0, Iteration: 56, Loss: 7.671154975891113\n",
      "Epoch 0, Iteration: 57, Loss: 8.066803932189941\n",
      "Epoch 0, Iteration: 58, Loss: 8.180681228637695\n",
      "Epoch 0, Iteration: 59, Loss: 8.254958152770996\n",
      "Epoch 0, Iteration: 60, Loss: 8.294692993164062\n",
      "b'I am the fool I have to the fool in thy the and the I shall in the blood I will not of that in a soldier in their in a soldier I do not in a fool and in the blood it the good the blood in their in their of and in their the and in his his his his the and the and the and the fool I shall And and in their the soul and that and thy the I shall and and that and thy the fool and and that I will in the soul of the and'\n",
      "Epoch 0, Iteration: 61, Loss: 8.232803344726562\n",
      "Epoch 0, Iteration: 62, Loss: 7.999847888946533\n",
      "Epoch 0, Iteration: 63, Loss: 7.976492404937744\n",
      "Epoch 0, Iteration: 64, Loss: 7.984496116638184\n",
      "Epoch 0, Iteration: 65, Loss: 8.357070922851562\n",
      "Epoch 0, Iteration: 66, Loss: 7.984153747558594\n",
      "Epoch 0, Iteration: 67, Loss: 7.994074821472168\n",
      "Epoch 0, Iteration: 68, Loss: 7.867952823638916\n",
      "Epoch 0, Iteration: 69, Loss: 7.95955753326416\n",
      "Epoch 0, Iteration: 70, Loss: 7.56326961517334\n",
      "Epoch 0, Iteration: 71, Loss: 7.8716583251953125\n",
      "Epoch 0, Iteration: 72, Loss: 8.103533744812012\n",
      "Epoch 0, Iteration: 73, Loss: 8.287284851074219\n",
      "Epoch 0, Iteration: 74, Loss: 8.046781539916992\n",
      "Epoch 0, Iteration: 75, Loss: 7.925191879272461\n",
      "Epoch 0, Iteration: 76, Loss: 8.109868049621582\n",
      "Epoch 0, Iteration: 77, Loss: 7.9535231590271\n",
      "Epoch 0, Iteration: 78, Loss: 7.898436546325684\n",
      "Epoch 0, Iteration: 79, Loss: 8.209100723266602\n",
      "Epoch 0, Iteration: 80, Loss: 8.021523475646973\n",
      "b'I am the is not of of and of to the that is to be of to the is to be the is a that and that is a the of of I will to be of the of of I will to be gone to the that I have the of I have to I thought and and of the is to and of and and of the and of the is a that and I thought of the That I have in I will I am to you of and I will to my troth, of I am the that to'\n",
      "Epoch 0, Iteration: 81, Loss: 8.071805953979492\n",
      "Epoch 0, Iteration: 82, Loss: 7.761768817901611\n",
      "Epoch 0, Iteration: 83, Loss: 7.70664644241333\n",
      "Epoch 0, Iteration: 84, Loss: 7.700527191162109\n",
      "Epoch 0, Iteration: 85, Loss: 7.857889652252197\n",
      "Epoch 0, Iteration: 86, Loss: 7.992070198059082\n",
      "Epoch 0, Iteration: 87, Loss: 7.826071739196777\n",
      "Epoch 0, Iteration: 88, Loss: 8.205507278442383\n",
      "Epoch 0, Iteration: 89, Loss: 7.882669448852539\n",
      "Epoch 0, Iteration: 90, Loss: 7.853353023529053\n",
      "Epoch 0, Iteration: 91, Loss: 8.009541511535645\n",
      "Epoch 0, Iteration: 92, Loss: 7.721782684326172\n",
      "Epoch 0, Iteration: 93, Loss: 7.413436412811279\n",
      "Epoch 0, Iteration: 94, Loss: 7.596020221710205\n",
      "Epoch 0, Iteration: 95, Loss: 7.906864166259766\n",
      "Epoch 0, Iteration: 96, Loss: 7.831371307373047\n",
      "Epoch 0, Iteration: 97, Loss: 7.765986442565918\n",
      "Epoch 0, Iteration: 98, Loss: 7.8921027183532715\n",
      "Epoch 0, Iteration: 99, Loss: 7.708671569824219\n",
      "Epoch 0, Iteration: 100, Loss: 7.705872535705566\n",
      "b'I am I will the And a a in a a the and a a and and the chain you to be thy I am to thy I do of I am I see of in his name, you the chain and and to be Thou hast to thy the And in a and in I do and the chain you that you to my I see I have a and and to my the world I am and I will in his the world in I have my the judgment to thy and to the world to thy the state of the'\n",
      "Epoch 0, Iteration: 101, Loss: 7.764257907867432\n",
      "Epoch 0, Iteration: 102, Loss: 7.947982311248779\n",
      "Epoch 0, Iteration: 103, Loss: 7.507824420928955\n",
      "Epoch 0, Iteration: 104, Loss: 7.639869213104248\n",
      "Epoch 0, Iteration: 105, Loss: 7.699763298034668\n",
      "Epoch 0, Iteration: 106, Loss: 7.992426872253418\n",
      "Epoch 0, Iteration: 107, Loss: 7.487825870513916\n",
      "Epoch 0, Iteration: 108, Loss: 7.905616283416748\n",
      "Epoch 0, Iteration: 109, Loss: 7.956020355224609\n",
      "Epoch 0, Iteration: 110, Loss: 8.174405097961426\n",
      "Epoch 0, Iteration: 111, Loss: 7.592216968536377\n",
      "Epoch 0, Iteration: 112, Loss: 7.740086078643799\n",
      "Epoch 0, Iteration: 113, Loss: 7.690042018890381\n",
      "Epoch 0, Iteration: 114, Loss: 8.029838562011719\n",
      "Epoch 0, Iteration: 115, Loss: 7.940199375152588\n",
      "Epoch 0, Iteration: 116, Loss: 7.832132339477539\n",
      "Epoch 0, Iteration: 117, Loss: 7.344972610473633\n",
      "Epoch 0, Iteration: 118, Loss: 7.501739978790283\n",
      "Epoch 0, Iteration: 119, Loss: 7.64232873916626\n",
      "Epoch 0, Iteration: 120, Loss: 7.939029216766357\n",
      "b'I am of in a in the most the and of I shall in his LORD. ANTONY. in my heart the most of I have the I shall and the and I am a of and of in the I am the of and the of of the of in the two I have the and of I do I have a in a of and of and the of the of of and of I do not and and in your the most the of in a of the and I do you to be his the of and the most and'\n",
      "Epoch 0, Iteration: 121, Loss: 7.9599480628967285\n",
      "Epoch 0, Iteration: 122, Loss: 7.580846786499023\n",
      "Epoch 0, Iteration: 123, Loss: 7.97481107711792\n",
      "Epoch 0, Iteration: 124, Loss: 7.984396934509277\n",
      "Epoch 0, Iteration: 125, Loss: 7.770615577697754\n",
      "Epoch 0, Iteration: 126, Loss: 8.045181274414062\n",
      "Epoch 0, Iteration: 127, Loss: 7.9885945320129395\n",
      "Epoch 0, Iteration: 128, Loss: 7.704561710357666\n",
      "Epoch 0, Iteration: 129, Loss: 7.211727142333984\n",
      "Epoch 0, Iteration: 130, Loss: 7.594143390655518\n",
      "Epoch 0, Iteration: 131, Loss: 7.701817035675049\n",
      "Epoch 0, Iteration: 132, Loss: 7.340260982513428\n",
      "Epoch 0, Iteration: 133, Loss: 7.140608787536621\n",
      "Epoch 0, Iteration: 134, Loss: 7.906113624572754\n",
      "Epoch 0, Iteration: 135, Loss: 8.069215774536133\n",
      "Epoch 0, Iteration: 136, Loss: 8.011966705322266\n",
      "Epoch 0, Iteration: 137, Loss: 7.862977504730225\n",
      "Epoch 0, Iteration: 138, Loss: 7.1901936531066895\n",
      "Epoch 0, Iteration: 139, Loss: 7.029735565185547\n",
      "Epoch 0, Iteration: 140, Loss: 7.177656173706055\n",
      "b'I am and I know the gods I know the Roman to be in my heart to be in his I know my heart, my lord. Ham. and my father the first my father the time to the Roman I have not the Roman I am I am to be in the and I will the time was to a and the first the Roman and to the Roman to a man of my heart, to my heart and the gods to be in your the time was and I will I do he is I know and my lord. and the Roman'\n",
      "Epoch 0, Iteration: 141, Loss: 7.4499592781066895\n",
      "Epoch 0, Iteration: 142, Loss: 7.9160614013671875\n",
      "Epoch 0, Iteration: 143, Loss: 7.6943206787109375\n",
      "Epoch 0, Iteration: 144, Loss: 8.147307395935059\n",
      "Epoch 0, Iteration: 145, Loss: 7.74329948425293\n",
      "Epoch 0, Iteration: 146, Loss: 7.686002254486084\n",
      "Epoch 0, Iteration: 147, Loss: 7.770028591156006\n",
      "Epoch 0, Iteration: 148, Loss: 7.939290523529053\n",
      "Epoch 0, Iteration: 149, Loss: 8.110458374023438\n",
      "Epoch 0, Iteration: 150, Loss: 7.980906963348389\n",
      "Epoch 0, Iteration: 151, Loss: 8.008225440979004\n",
      "Epoch 0, Iteration: 152, Loss: 7.955791473388672\n",
      "Epoch 0, Iteration: 153, Loss: 8.118045806884766\n",
      "Epoch 0, Iteration: 154, Loss: 8.153851509094238\n",
      "Epoch 0, Iteration: 155, Loss: 7.822810649871826\n",
      "Epoch 0, Iteration: 156, Loss: 8.065927505493164\n",
      "Epoch 0, Iteration: 157, Loss: 7.707730770111084\n",
      "Epoch 0, Iteration: 158, Loss: 6.627529144287109\n",
      "Epoch 0, Iteration: 159, Loss: 6.911524772644043\n",
      "Epoch 0, Iteration: 160, Loss: 7.545656204223633\n",
      "b'I am of the I and I have been the which and to be the which and the which I and and I am to your the and and to your of to the which and to your to your the of to his life of the and the to my lord, I am the which of and to be the and of to your of the and of and to be DUKE I and the to my are to his and I will of I do I have not to the I am the and I have been the of to the'\n",
      "Epoch 0, Iteration: 161, Loss: 7.5916666984558105\n",
      "Epoch 0, Iteration: 162, Loss: 7.305614471435547\n",
      "Epoch 0, Iteration: 163, Loss: 7.04335355758667\n",
      "Epoch 0, Iteration: 164, Loss: 7.341814041137695\n",
      "Epoch 0, Iteration: 165, Loss: 7.271373271942139\n",
      "Epoch 0, Iteration: 166, Loss: 7.836933135986328\n",
      "Epoch 0, Iteration: 167, Loss: 7.620101451873779\n",
      "Epoch 0, Iteration: 168, Loss: 7.845081329345703\n",
      "Epoch 0, Iteration: 169, Loss: 7.625881195068359\n",
      "Epoch 0, Iteration: 170, Loss: 7.64795446395874\n",
      "Epoch 0, Iteration: 171, Loss: 7.736456871032715\n",
      "Epoch 0, Iteration: 172, Loss: 7.592274188995361\n",
      "Epoch 0, Iteration: 173, Loss: 7.779494285583496\n",
      "Epoch 0, Iteration: 174, Loss: 7.636693954467773\n",
      "Epoch 0, Iteration: 175, Loss: 7.797094821929932\n",
      "Epoch 0, Iteration: 176, Loss: 7.226490020751953\n",
      "Epoch 0, Iteration: 177, Loss: 7.116242408752441\n",
      "Epoch 0, Iteration: 178, Loss: 7.047044277191162\n",
      "Epoch 0, Iteration: 179, Loss: 7.159438610076904\n",
      "Epoch 0, Iteration: 180, Loss: 7.755255222320557\n",
      "b'I am and to your to my love of and of and of the of to my lord. And the King the I for to be and I am of and the King of the I am of I am and and to your and of and to his I have not to be and I have I am of and I am the I will of and and and of of to my lord. And the the the the the the I am to be the to be to your of the the and the and of the King to my lord.'\n",
      "Epoch 0, Iteration: 181, Loss: 7.655158996582031\n",
      "Epoch 0, Iteration: 182, Loss: 7.494095325469971\n",
      "Epoch 0, Iteration: 183, Loss: 7.461838722229004\n",
      "Epoch 0, Iteration: 184, Loss: 8.030317306518555\n",
      "Epoch 0, Iteration: 185, Loss: 8.001951217651367\n",
      "Epoch 0, Iteration: 186, Loss: 7.81628942489624\n",
      "Epoch 0, Iteration: 187, Loss: 7.321592330932617\n",
      "Epoch 0, Iteration: 188, Loss: 7.499628067016602\n",
      "Epoch 0, Iteration: 189, Loss: 7.5968122482299805\n",
      "Epoch 0, Iteration: 190, Loss: 8.267779350280762\n",
      "Epoch 0, Iteration: 191, Loss: 7.958438873291016\n",
      "Epoch 0, Iteration: 192, Loss: 7.750503063201904\n",
      "Epoch 0, Iteration: 193, Loss: 7.848876476287842\n",
      "Epoch 0, Iteration: 194, Loss: 7.70833158493042\n",
      "Epoch 0, Iteration: 195, Loss: 7.755958080291748\n",
      "Epoch 0, Iteration: 196, Loss: 7.989834308624268\n",
      "Epoch 0, Iteration: 197, Loss: 7.890469551086426\n",
      "Epoch 0, Iteration: 198, Loss: 7.791647434234619\n",
      "Epoch 0, Iteration: 199, Loss: 7.868039608001709\n",
      "Epoch 0, Iteration: 200, Loss: 7.949747085571289\n",
      "b'I am the and of and I do of the I am and to the I will to our cold I am of to our brother and and to the I have found it. And of and of of I have no I have no to our brother to our and and of I do and to my love I do I have no the world and the and and and of of I do I will the and to my love and to your to be of the time to my self the best the ADRIANA. to be a to be in'\n",
      "Epoch 0, Iteration: 201, Loss: 7.665311813354492\n",
      "Epoch 0, Iteration: 202, Loss: 7.4788689613342285\n",
      "Epoch 0, Iteration: 203, Loss: 7.477094650268555\n",
      "Epoch 0, Iteration: 204, Loss: 7.544727802276611\n",
      "Epoch 0, Iteration: 205, Loss: 7.818498611450195\n",
      "Epoch 0, Iteration: 206, Loss: 8.150015830993652\n",
      "Epoch 0, Iteration: 207, Loss: 7.8649773597717285\n",
      "Epoch 0, Iteration: 208, Loss: 8.013117790222168\n",
      "Epoch 0, Iteration: 209, Loss: 7.688543796539307\n",
      "Epoch 0, Iteration: 210, Loss: 7.845788955688477\n",
      "Epoch 0, Iteration: 211, Loss: 7.859037399291992\n",
      "Epoch 0, Iteration: 212, Loss: 7.888535022735596\n",
      "Epoch 0, Iteration: 213, Loss: 7.626379489898682\n",
      "Epoch 0, Iteration: 214, Loss: 7.9015655517578125\n",
      "Epoch 0, Iteration: 215, Loss: 7.427014350891113\n",
      "Epoch 0, Iteration: 216, Loss: 7.772697925567627\n",
      "Epoch 0, Iteration: 217, Loss: 7.6189374923706055\n",
      "Epoch 0, Iteration: 218, Loss: 8.002237319946289\n",
      "Epoch 0, Iteration: 219, Loss: 7.548611164093018\n",
      "Epoch 0, Iteration: 220, Loss: 7.891674041748047\n",
      "b'I am you be of to my friend and you are not you have a of and to your to my lord, and you shall a the I would you are to your to my friend and you are my are not you are my friend of to my lord, and the of to my friend not I had not you have you not you not you are my lord, and to your the is to your and I have a to a to a the of I had shall not the and the very to your not you not you be of'\n",
      "Epoch 0, Iteration: 221, Loss: 7.549992084503174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-94156e6beef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Remove state from graph for gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 942\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    batchs = getBatchs(\n",
    "        data.get('batchs'),\n",
    "        data.get('labels'),\n",
    "        batchSize,\n",
    "        sequenceSize\n",
    "    )\n",
    "    \n",
    "    stateHidden, stateMemory = model.resetState(batchSize)\n",
    "    \n",
    "    if torch.cuda.is_available and cuda:\n",
    "        stateHidden, stateMemory = stateHidden.cuda(), stateMemory.cuda()\n",
    "            \n",
    "    for batch_data, batch_label in batchs:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Set train mode\n",
    "        model.train()\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transform array to tensor\n",
    "        batch_data = torch.tensor(batch_data)\n",
    "        \n",
    "        batch_label = torch.tensor(batch_label)\n",
    "        \n",
    "        # Send tensor to GPU\n",
    "        if torch.cuda.is_available and cuda:\n",
    "            batch_data = batch_data.cuda()\n",
    "            \n",
    "            batch_label = batch_label.cuda()\n",
    "        \n",
    "        # Train\n",
    "        logits, (stateHidden, stateMemory) = model(\n",
    "            batch_data,\n",
    "            (stateHidden, stateMemory)\n",
    "        )\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits.transpose(1, 2), batch_label)\n",
    "        \n",
    "        # Remove state from graph for gradient clipping\n",
    "        stateHidden = stateHidden.detach()\n",
    "        \n",
    "        stateMemory = stateMemory.detach()\n",
    "        \n",
    "        # Back-propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (inline)\n",
    "        nn.utils.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            gradientsNorm\n",
    "        )\n",
    "        \n",
    "        # Update network's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Loss value\n",
    "        print(f'Epoch {epoch}, Iteration: {iteration}, Loss: {loss.item()}')\n",
    "        \n",
    "        # Predict value\n",
    "        if iteration % 20 == 0:\n",
    "            predict(model, initialWords, data.get('numberOfWords'), data.get('wordsToInt'), data.get('intToWords'), top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
